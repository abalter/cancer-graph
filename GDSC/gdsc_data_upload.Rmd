---
title: "Upload GDSC data to BQ"
author: "Ariel Balter"
date: "10/14/2021"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

https://stackoverflow.com/questions/42426914/error-in-vapply-values-must-be-length-1-but-funx11-result-is-length-0

bq_table_upload(x=players_table, values= players_df_2, create_disposition='CREATE_IF_NEEDED', write_disposition='WRITE_APPEND')

https://rdrr.io/cran/bigrquery/man/api-perform.html

# Knitr
```{r}
knitr::opts_chunk$set(
  warning=F,
  message=F,
  echo=F,
  error=F,
  number_section=T,
  include=T
)
```

# Libraries
```{r}
library(tidyverse)
library(magrittr)
library(knitr)
library(DT)
library(here)
library(DBI)
library(bigrquery)
library(jsonlite)
library(openxlsx)
library(glue)
library(vroom)
library(readr)
```


# Utility Functions
## Load Schema Table
This is needed for some utility functions that create colClasses and bq_field objects from the dataframe columns. That schema tables uas four columns:

* name: variable name
* type: variable type in BigQuery version, but lowercase for some
  reason(?):
  - string
  - float
  - integer
  - boolean
* mode: BigQuery mode. I've made all of the either "nullable" or
  "repeated".
* Description: I tried to put in a lot from documentation, papers, and
  my own understanding. There is much room for filling them in. 
  
```{r}
schema_table = 
  read.xlsx('gdsc_data_schema.xlsx') %>% 
  mutate(across(everything(), trimws))
```

## Custom table printing
```{r}
display_table = function(
  df,
  caption=NULL,
  search=list(regex = TRUE, caseInsensitive = TRUE)
)
{
  num_rows = nrow(df)
  df %>%
    datatable(
      extensions = c('ColReorder', 'Scroller'),
      options = list(
        colReorder=T,
        scroller=T,
        scrollY=300,
        sScrollX=T,
        searchHighlight = TRUE,
        search = search,

        pageLength = num_rows
      ),
      caption = htmltools::tags$caption(
        style = '
        caption-side: top;
        text-align: center;
        color:black;
        font-size:150% ;',
        caption),
      # caption = caption,
      rownames = T
    ) %>%
    formatStyle(columns = colnames(.), fontSize = '50%')
}

```


## Create Col Classes and BQ Fields
```{r}
getColClasses = function(df)
{
  
  ### Use Schema Table and dataframe columns
  ### to create a list of column classes to use
  ### when reading in the data to set data types.
  
  
  ### For testing
  # df = temp_data
  
  col_classes = list()
  cols = colnames(df)
  
  
  class_table = 
    schema_table %>% 
    filter(name %in% cols) %>% 
    mutate(type = 
      case_when(
        type == "float" ~ "numeric",
        type == "string" ~ "character",
        type == "integer" ~ "integer",
        type == "date" | type == "datetime" ~ "datetime"
      )
    ) %>% 
    select(name, type)

  
  col_classes = sapply(
    cols, 
    simplify=F, 
    function(colname)
    {
      class_table %>% 
        filter(name==colname) %>% 
        pull(type) %>% 
        return()
    }
  )
  
  return(col_classes)
}


getBqFieldList = function(df)
{
  
    
  ### Use Schema Table and dataframe columns
  ### to create a list of `bq_field` objects which
  ### `bigrauery` uses to set schema for uploading
  ### to BigQuery

  ### For Testing
  # df = temp_data
  
  cols = colnames(df)
  
  bq_field_list = 
    schema_table %>% 
    filter(name %in% cols) %>% 
    mutate(type = toupper(type)) %>% 
    rowwise() %>% 
    mutate(bq_field = list(bq_field(
        name = name,
        type = type,
        mode = mode,
        description = description
      ))
    ) %>% 
    pull(bq_field)
    # select(name, bq_field)z
  
  ### Apply the name field of the bq_field to be the name
  ### in the list
  # names(bq_field_list) = sapply(bq_field_list, function(x) x$name)
  
  return(bq_field_list)
}
```

## Array handling functions
```{r}

arrayStringToStringArray = function(data)
{
  ### BigRQuery says posts an error that array fields need to 
  ### be uploaded as JSON strings. However, it only actually
  ### "works" if the values are an R character array. This
  ### function converts a value like "a, b, c" to 
  ### c("a", "b", "c")
  
  data = 
    data %>%
    replace_na(., "") %>% 
    str_split(., ",") %>% 
    lapply(str_trim)
  
  return(data)
}

arrayStringToJSON = function(data)
{
  
  ### BigRQuery says posts an error that array fields need to 
  ### be uploaded as JSON strings. However, it only actually
  ### "works" if the values are an R character array. This
  ### function converts a value like "a, b, c" to a JSON
  ### string like ["a", "b", "c"]. However this still generates
  ### an error.

  data = 
    data %>%
    replace_na(., "") %>% 
    str_split(., ",") %>% 
    lapply(str_trim) %>% 
    unlist() %>% 
    toJSON()
  
  return(data)
}


type_map = list(
  INTEGER = "integer",
  FLOAT = "double",
  STRING = "character"
)


bqFieldsToColClasses = function(field_list)
{
  
  ### Utility to convert bq_field objects to colClasses
  ### for read.delim. If you need it.
  
  # print(fields)
  
  col_classes = lapply(field_list, function(field)
  {
    type_map[[field$type]];
  })
  
  names(col_classes) = lapply(field_list, function(field)
  {
    field$name
  })
  
  # print(col_classezs)
  
  return(col_classes)
}
```


## Upload to BQ Function
I didn't end up actually using this.
```{r}

uploadToBQ = function(filename, schema, sep=",")
{
  
  repeated_fields = ""
    
  df = read.csv(
    file=here(filename),
    colClasses = bqFieldsToColClasses(schema),
    stringsAsFactors = F,
    sep=sep
  ) %>% 
  mutate(across(!repeated_fields, arrayStringToStringArray))
  
  table_bq = bq_table(
    project = gdsc_dataset$project, 
    dataset = gdsc_dataset$dataset, 
    table = basename(filename) %>% str_remove(".csv")
  )
  
  if (bq_table_exists(table_bq))
  {
    bq_table_delete(table_bq)
  }
  
  bq_table_upload(
    table_bq,
    quiet = FALSE,
    values = df,
    fields=schema
  )
}

uploadToBQ = function(df, table_name, schema, sep=",")
{
  
  repeated_fields = ""
    
  # df = read.csv(
  #   file=here(filename),
  #   colClasses = bqFieldsToColClasses(schema),
  #   stringsAsFactors = F,
  #   sep=sep
  # ) %>% 
  # mutate(across(!repeated_fields, arrayStringToStringArray))
  
  table_bq = bq_table(
    project = gdsc_dataset$project, 
    dataset = gdsc_dataset$dataset, 
    table = table_name
  )
  
  if (bq_table_exists(table_bq))
  {
    bq_table_delete(table_bq)
  }
  
  bq_table_upload(
    table_bq,
    quiet = FALSE,
    values = df,
    fields=schema
  )
}
```


# Set Up BigQuery
```{r}
bq_deauth()
bq_auth(email="ariel.balter@gmail.com")

# cancer_graph_project = "psjh-238522"
cancer_graph_project = "some-os-bio-data"
dataset = "gdsc_raw"
```


# Create GDSC dataset
```{r}
gdsc_dataset = bq_dataset(cancer_graph_project, dataset)

if (bq_dataset_exists(gdsc_dataset))
{
  bq_dataset_delete(gdsc_dataset, delete_contents = T)
}

bq_dataset_create(gdsc_dataset)


gdsc_conn = dbConnect(
  bigrquery::bigquery(),
  project = cancer_graph_project,
  dataset = dataset
)
```

# GDSC Files
```{r}
prefix = "large_files/harmonized"

gdsc_file_paths = c(
  file.path(prefix, "cell_lines_metadata.tsv"),
  file.path(prefix, "drug_metadata.tsv"),
  file.path(prefix, "gdsc1_anova_all_tcga_results.tsv"),
  file.path(prefix, "gdsc1_anova_pancancer_results.tsv"),
  file.path(prefix, "gdsc1_drug_ic50_results.tsv"),
  file.path(prefix, "gdsc2_anova_all_tcga_results.tsv"),
  file.path(prefix, "gdsc2_anova_pancancer_results.tsv"),
  file.path(prefix, "gdsc2_drug_ic50_results.tsv"),
  file.path(prefix, "genetic_features_all_tcga_metadata.tsv"),
  file.path(prefix, "genetic_features_all_tcga_results.tsv"),
  file.path(prefix, "genetic_features_pancancer_metadata.tsv"),
  file.path(prefix, "genetic_features_pancancer_results.tsv")
)

gdsc_filename_list = sapply(gdsc_file_paths, basename, USE.NAMES = F)

print(gdsc_filename_list)
```


# Create Tables
## Create BQ Table List
```{r}

gdsc_table_names = 
  gdsc_file_paths %>% 
  basename() %>% 
  str_remove(".tsv") %>% 
  ### Apply array values as names
  set_names(.,.) %>% 
  as.list()

gdsc_bq_tables_list =
  lapply(gdsc_table_names, function(table_name)
  {
    
    bq_table(
      gdsc_dataset$project, 
      gdsc_dataset$dataset, 
      table_name
    ) %>% 
    return()
  })

```

## Reset (delete and recreate) BQ Tables
```{}
for (gdsc_bq_table in gdsc_bq_tables_list)
{
  
  glue(
    "project: {project}, dataset: {dataset}, table_name: {table}", 
    .envir = gdsc_bq_table
  ) %>% 
  print()
  
  if (bq_table_exists(gdsc_bq_table))
  {
    bq_table_delete(gdsc_bq_table)
  }

  bq_table_create(gdsc_bq_table)
  
}
```


# Upload Data with Schema
```{r}

for (gdsc_bq_table in gdsc_bq_tables_list)
# for (gdsc_filename in gdsc_filename_list)
{
  
  
  filename = file.path(prefix, glue(gdsc_bq_table$table, ".tsv"))
  print(filename)
  
  glue(
    "project: {gdsc_bq_table$project}, 
    dataset: {gdsc_bq_table$dataset}, 
    table_name: {gdsc_bq_table$table}, 
    filename: {filename}"
  ) %>% 
  print()
  
  ### Read just one line to build colclasses and bq fields
  temp_data = read_tsv(filename, n_max=1)
  
  ### Get col classes for reading in the data with types
  col_classes = getColClasses(temp_data)
  ### get bq schema to convert REPEATED data to proper
  ### arrays.
  bq_field_list = getBqFieldList(temp_data)

  ### Read the data
  data = read.delim(
    filename,
    colClasses = col_classes,
    stringsAsFactors = F,
    sep = "\t"
  )
  
  ### get list of repeated columns
  array_cols = lapply(
    bq_field_list,
    function(x)
    {
      # print(x$mode)
      if (toupper(x$mode) == "REPEATED")
      {
        return(x$name)
      }
    }
  ) %>% unlist()
  
  ### Convert array cols to proper format.
  ### Basically, if the data is a list of words
  ### like "EGFR, VEGF, ALK" you get c("EGFR", "VEGF", "ALK").
  ### Bigquery says it wants JSON strings, but using toJSON
  ### makes it give a JSON error! So just an array of strings
  ### is fine.
  data = 
    data %>% 
    mutate(across(array_cols, arrayStringToStringArray))
  
  # print(col_classes)
  bq_field_list = getBqFieldList(data) %>% unname()
  
  if (bq_table_exists(gdsc_bq_table))
  {
    print(gdsc_bq_table$table) 
    bq_table_delete(gdsc_bq_table)
  }
  
  bq_table_upload(
    gdsc_bq_table,
    values = data,
    quiet = FALSE,
    fields = bq_field_list
  )
  
  
  gc()
  
}


```



